{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03f924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Change to wherever you've copied the repo.\n",
    "sys.path.insert(1, '/home/richard/nfmc_jax/')\n",
    "import nfmc_jax\n",
    "import arviz as az\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import multivariate_normal as n_mvn\n",
    "import chaospy\n",
    "import corner\n",
    "import torch\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import getdist\n",
    "from getdist import plots, MCSamples\n",
    "\n",
    "seed=1234\n",
    "np.random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a42861",
   "metadata": {},
   "source": [
    "# Sampling a 10d correlated Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c4b90",
   "metadata": {},
   "source": [
    "I'm just taking the primer notebook I put together and using the 10d Gaussian as a testbed for the weighted PCA. This notebook is a little neater!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0dbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by obtaining some prior samples. I've chosen Unif(-1, 1) in all dimensions.\n",
    "\n",
    "n = 10 # Number of dimensions.\n",
    "n_prior = 200 # Number of prior samples\n",
    "# Note the use of the key here. Jax uses this as opposed to your normal random seed. \n",
    "jax_prior_init = jax.random.uniform(key, shape=(n_prior, n), minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb45741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSD:  True\n",
      "Condition number:  199.08320751596906\n",
      "Op norm:  0.1587924033016887\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is just a function to generate a covariance matrix for the Gaussian as a random sample from a Wishart\n",
    "distribution, whilst ensuring the condition number is not too high.\n",
    "'''\n",
    "def get_icov(n, target=200, iseed=seed, eps=1, scale=50, just_do_it=False):\n",
    "    condition = 0\n",
    "    eigmax = np.inf\n",
    "    this_seed = iseed\n",
    "    if just_do_it:\n",
    "        wish = scipy.stats.wishart(df=n, scale=np.eye(n)*scale, seed=this_seed)\n",
    "        iC = wish.rvs(size=1)\n",
    "        C = np.linalg.inv(iC)\n",
    "        eigs = np.linalg.eigvals(C)\n",
    "        eigmax,eigmin = eigs.max(), eigs.min()\n",
    "        condition = eigmax / eigmin\n",
    "    elif not just_do_it:\n",
    "        while(abs(condition-target)>eps):\n",
    "            wish = scipy.stats.wishart(df=n, scale=np.eye(n)*scale,seed=this_seed)\n",
    "            iC = wish.rvs(size=1)\n",
    "            C = np.linalg.inv(iC)\n",
    "            eigs = np.linalg.eigvals(C)\n",
    "            eigmax,eigmin = eigs.max(),eigs.min()\n",
    "            condition = eigmax / eigmin\n",
    "            this_seed += 1\n",
    "    print(\"PSD: \",np.all(eigs>0))\n",
    "    print(\"Condition number: \", condition)\n",
    "    print(\"Op norm: \", eigmax)\n",
    "    return iC, C\n",
    "\n",
    "iCov, Cov = get_icov(n, eps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0321d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function that returns the log-likelihood. First argument must be the variables you want to sample,\n",
    "followed by any additional arguments.\n",
    "'''\n",
    "def log_like_cg(x, mu_diag=0, icov=iCov):\n",
    "    n=x.shape[-1]\n",
    "    mu = mu_diag * jnp.ones(n)\n",
    "    return - 0.5 * jnp.dot((x - mu), jnp.dot(icov, (x - mu)))\n",
    "'''\n",
    "Define a function that returns the log-prior. As before, the first argument must the the variables you want to \n",
    "sample, followed by any additional arguments.\n",
    "'''\n",
    "def log_flat_prior(x, lower=-1, upper=1):\n",
    "    return jnp.sum(jax.scipy.stats.uniform.logpdf(x, loc=lower, scale=upper-lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31b7ac",
   "metadata": {},
   "source": [
    "Below I have defined the key parameters that I think you will need to worry about. If you look at the actual code you'll see a lot more parameters. Some of these are from older versions of the algorithm, and some of these are only relevant for the NFO optimization algorithm. We'll tidy this up at some point, but this should be enough to get you going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c102741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "General parameters\n",
    "------------------\n",
    "In addition to the log-likelihood and prior functions defined above that you'll pass, we have:\n",
    "\n",
    "1. log_like_args: Any additional arguments to be passed to the log-likelihood function as a tuple object.\n",
    "2. log_prior_args: As above but for the prior. Not necessary here, but I've included arguments for illustration.\n",
    "3. inference_mode: Either 'optimization' or 'sampling'. You'll want 'sampling'.\n",
    "4. vmap: If True we evaluate the log-likelihood and prior functions using a vectorizing map (i.e. jax.vmap). \n",
    "   If False we parallelise the computation over available cores. The parallelisation has been tested on NERSC, \n",
    "   but I've pretty much exclusively been using vmap (you may want to parallelise if you have some very expensive \n",
    "   likelihoods).\n",
    "5. parallel_backend: If using parallel evaluations, choose the parallel backend. Can be set to Pool, Client or\n",
    "   MapReduce. Will use multiprocess, dask or sharedmem respectively. If left as None, will use multiprocess.\n",
    "6. bounds: Any hard parameter bounds to pass to SINF/SNF. Passed as array([lower_bounds, upper_bounds]). None\n",
    "   if no hard bounds.\n",
    "7. k_trunc: Clipping parameter for importance weights. IW clipped at <IW>*N^k_trunc. Note, not currently used\n",
    "   with SMC-type sampling.\n",
    "8. random_seed: Random seed for inference.\n",
    "'''\n",
    "log_like_args = ()\n",
    "log_prior_args = ((jnp.array([-1]), jnp.array([1]))) \n",
    "inference_mode = 'sampling'\n",
    "vmap = True\n",
    "parallel_backend = None\n",
    "bounds = np.array([-np.ones(n), np.ones(n)])\n",
    "k_trunc = 0.5\n",
    "random_seed = seed\n",
    "\n",
    "'''\n",
    "SINF/SNF parameters\n",
    "---------------\n",
    "Key SINF/SNF parameters. See GIS.py, SINF.py and optimize.py for code.\n",
    "\n",
    "1. alpha_w: Regularisation parameters for weighted q SINF fits. Passed as a tuple of two values between 0 and 1. \n",
    "   Closer to 1 will give more regularisation. If set to None, SINF uses cross-validation to choose a value.\n",
    "2. NBfirstlayer: Whether to use a Naive Bayes first layer in SINF.\n",
    "3. verbose: Whether you want verbose output from SINF.\n",
    "4. interp_nbin: Number of spline knots for rational quadratic splines in SINF. \n",
    "5. trainable_qw: Whether to run SNF after SINF. This was relevant when SINF and SNF were contained in the same\n",
    "   method. There are separate methods now, and I'm using this so I can more easily change things. You should\n",
    "   probably leave this as False for now. It won't break anything if you set it as True. Just means you'll run \n",
    "   SNF one more time than you think.\n",
    "6. sgd_steps: Number of stochastic gradient descent steps to take for SNF training.\n",
    "7. gamma: Parameter in loss function 1 (log E(q^(-gamma)*(p - Zq)^2)). \n",
    "8. knots_trainable: Only relevant if trainable_qw is True, in which case this overwrites interp_nbin.\n",
    "9. optimize_directions: Whether we optimize directions in SNF.\n",
    "10. logp_cut: logp threshold to be applied during SNF training. Currently I don't impose a cut.\n",
    "11. edge_bins: Number of spline knots at the boundary.\n",
    "12. Whiten: Whether to include a whitening layer for SINF.\n",
    "13. iteration: Maximum number of SINF layers.\n",
    "14. NERCOME: Whether to use the NERCOME algorithm for SINF.\n",
    "'''\n",
    "#sinf parameters\n",
    "alpha_w = (0.9, 0.9)\n",
    "NBfirstlayer = True\n",
    "verbose = False\n",
    "interp_nbin = 5\n",
    "iteration = 5\n",
    "trainable_qw = False # Just leave this for now.\n",
    "sgd_steps = 10\n",
    "gamma = 0\n",
    "knots_trainable = 5\n",
    "optimize_directions = False\n",
    "logp_cut = None\n",
    "edge_bins = 0\n",
    "Whiten = True\n",
    "iteration = 5\n",
    "NERCOME = True\n",
    "\n",
    "'''\n",
    "SMC parameters\n",
    "--------------\n",
    "Parameters used for SMC and MH exploration strategies.\n",
    "\n",
    "1. nfmc_frac_validate: Fraction of samples used in validation for SINF/SNF.\n",
    "2. min_delta_beta_init: Minimum relative change in beta at start.\n",
    "3. min_delta_beta_final: Minimum relative change in beta at end. delta_beta decays to this between start and end.\n",
    "4. snf_loss: Specify the SNF loss function. (0) E(q^(-gamma)*(p - Zq)^2), (1) log E(q^(-gamma)*(p - Zq)^2),\n",
    "   (2) sum((logp-logq-logZ)^2), (3) -E(logq), (4) log(E((q/q_uw)(p/q - Z)^2)).\n",
    "5. nfmc_snf_only: Whether to only use trainable SNF updates to q after the first q fit.\n",
    "6. fixed_beta_schedule: Can specify a fixed beta schedule as a list of beta values.\n",
    "7. vanilla_smc: Whether to use the SMC+MH exploration strategy. This is currently the focus for sampling.\n",
    "   Old option was to intialise using NFO optimization.\n",
    "8. reg: SNF regularisation parameter.\n",
    "9. reg1: SNF regularisation parameter.\n",
    "10. mh_mode: Specify 'IMH' for independent MH exploration, 'CMH' for latent space MH, 'both' for using both.\n",
    "    CMH is still being tested so stick with 'IMH' for now.\n",
    "11. imh_target_acc_rate: Target acceptance fraction for original samples in IMH.\n",
    "12. max_imh_steps: Maximum number of IMH iterations at a given temperature level.\n",
    "13. cmh_target_acc_rate: Target acceptance rate for CMH exploration.\n",
    "14. max_cmh_steps: Maximum number of steps in a CMH chain.\n",
    "15. cmh_p_acc_rate: Parameter used for tuning number of CMH steps. Between 0 and 1. Closer to 1 results in more\n",
    "    steps.\n",
    "16. sinf_resampled: Whether to fit SINF using re-sampled samples and uniform weight.\n",
    "'''\n",
    "nfmc_frac_validate = 0.1\n",
    "min_delta_beta_init = 0.01\n",
    "min_delta_beta_final = 0.01\n",
    "snf_loss = 2\n",
    "nfmc_snf_only = False\n",
    "fixed_beta_schedule = None\n",
    "vanilla_smc = True\n",
    "reg = 0.9\n",
    "reg1 = 0.9\n",
    "mh_mode = 'CMH'\n",
    "imh_target_acc_rate = 0.5\n",
    "max_imh_steps = 25\n",
    "cmh_target_acc_rate = 0.234\n",
    "max_cmh_steps = 25\n",
    "cmh_p_acc_rate = 0.85\n",
    "sinf_resampled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4079c",
   "metadata": {},
   "source": [
    "Now we can run our 10d Gaussian example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0f5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference mode is sampling. Maximum beta is set to 1.\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.0037841796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/nfmc_jax/nfmc_jax/nfomc/nfomc.py:383: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448222085/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  weight_train=torch.from_numpy(np.asarray(self.train_weights[fit_idx, ...])),\n",
      "/home/richard/nfmc_jax/nfmc_jax/sinf/SINF.py:292: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448222085/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2500.)\n",
      "  D, E = torch.symeig(covariance, eigenvectors=True)\n",
      "/home/richard/nfmc_jax/nfmc_jax/sinf/SINF.py:495: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448222085/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:1940.)\n",
      "  Q, R = torch.qr(ATi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for trainable qw = 0.1184924349999994\n",
      "Running latent space MH sampling for 25 steps\n",
      "Chain acceptance rate = [0.2  0.36 0.08 0.04 0.08 0.16 0.28 0.32 0.36 0.24 0.04 0.04 0.04 0.12\n",
      " 0.16 0.28 0.04 0.2  0.24 0.08 0.6  0.2  0.6  0.24 0.4  0.2  0.   0.\n",
      " 0.48 0.2  0.24 0.24 0.56 0.04 0.12 0.04 0.28 0.08 0.08 0.12 0.   0.68\n",
      " 0.   0.08 0.24 0.32 0.04 0.48 0.   0.36 0.   0.04 0.36 0.16 0.16 0.24\n",
      " 0.24 0.28 0.04 0.2  0.   0.04 0.16 0.4  0.04 0.52 0.36 0.08 0.32 0.2\n",
      " 0.   0.24 0.32 0.08 0.24 0.12 0.36 0.04 0.16 0.08 0.64 0.12 0.   0.12\n",
      " 0.4  0.04 0.24 0.04 0.04 0.04 0.36 0.28 0.56 0.04 0.08 0.6  0.04 0.4\n",
      " 0.24 0.36 0.2  0.12 0.24 0.04 0.24 0.16 0.   0.32 0.08 0.16 0.24 0.12\n",
      " 0.04 0.16 0.44 0.04 0.2  0.   0.   0.   0.32 0.24 0.2  0.24 0.08 0.44\n",
      " 0.68 0.44 0.16 0.16 0.12 0.2  0.08 0.32 0.08 0.12 0.24 0.4  0.32 0.16\n",
      " 0.2  0.   0.04 0.04 0.   0.28 0.   0.04 0.28 0.2  0.04 0.2  0.04 0.04\n",
      " 0.24 0.08 0.12 0.04 0.4  0.52 0.4  0.   0.   0.48 0.04 0.08 0.2  0.2\n",
      " 0.08 0.16 0.16 0.4  0.04 0.04 0.16 0.12 0.08 0.72 0.16 0.16 0.04 0.56\n",
      " 0.44 0.   0.48 0.04 0.32 0.44 0.12 0.08 0.52 0.24 0.08 0.04 0.08 0.\n",
      " 0.   0.   0.   0.24]\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.011033620685338974\n",
      "Time taken for trainable qw = 0.2379639470000008\n",
      "Running latent space MH sampling for 13 steps\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Too many indices for array: 1 non-None/Ellipsis indices for dim 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3b459e54a4fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trace = nfmc_jax.infer_nfomc(log_like_func=log_like_cg, \n\u001b[0m\u001b[1;32m      2\u001b[0m                              \u001b[0mlog_prior_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_flat_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              \u001b[0minit_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjax_prior_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              \u001b[0mlog_prior_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_prior_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0minference_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nfmc_jax/nfmc_jax/nfomc/infer_nfomc.py\u001b[0m in \u001b[0;36minfer_nfomc\u001b[0;34m(log_like_func, log_prior_func, init_samples, log_like_args, log_prior_args, vmap, parallel_backend, inference_mode, n0, k_trunc, N, t_ess, g_AF, N_AF, expl_top_AF, vanilla_smc, local_smc, mh_mode, imh_target_acc_rate, max_imh_steps, cmh_target_acc_rate, max_cmh_steps, cmh_p_acc_rate, expl_latent, expl_top_qw, beta_max, min_delta_beta_init, min_delta_beta_final, fixed_beta_schedule, rel_beta, frac_rel_beta_AF, latent_sigma, use_latent_beta2, use_pq_beta_IW1, bounds, N_temp, eps_z, nf_iter, local_thresh, nfmc_draws, cull_mode, cull_lowp_tol, cull_iw_tol, max_cull_frac, agressive_cull, ess_tol, nfmc_frac_validate, nfmc_local_exploration, local_step_factor, nfmc_AF_samples, nfmc_full_qw_samples, nfmc_snf_only, frac_validate, iteration, alpha_w, alpha_uw, verbose, top_verbose, n_component, interp_nbin, KDE, bw_factor_min, bw_factor_max, bw_factor_num, rel_bw, edge_bins, ndata_wT, MSWD_max_iter, NBfirstlayer, logit, Whiten, trainable_qw, trainable_quw, sgd_steps, gamma, knots_trainable, snf_loss, optimize_directions, reg, reg1, logp_cut, train_updates, batchsize, nocuda, patch, shape, device, random_seed)\u001b[0m\n\u001b[1;32m    415\u001b[0m             ) = zip(*sample_results)\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mvanilla_smc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0msample_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample_nfmc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nfmc_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             (\n\u001b[1;32m    419\u001b[0m                 \u001b[0mlog_evidence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nfmc_jax/nfmc_jax/nfomc/infer_nfomc.py\u001b[0m in \u001b[0;36msample_nfmc_int\u001b[0;34m(nfomc, vanilla_smc, mh_mode, local_smc, logp_cut, eps_z, ess_tol, nf_iter, nfmc_draws, nfmc_frac_validate, nfmc_local_exploration, nfmc_snf_only, cull_mode, agressive_cull, device, _nfmc_log)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                                    cmh_sample_density=old_beta * nfomc.cmh_likelihood_logp + nfomc.cmh_prior_logp)\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0mnfomc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_nf_mhsmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             save_dicts(nfomc, stage, _nfmc_log, iter_sample_dict, iter_weight_dict, iter_logp_dict, iter_logZ_dict,\n",
      "\u001b[0;32m~/nfmc_jax/nfmc_jax/nfomc/nfomc.py\u001b[0m in \u001b[0;36mupdate_nf_mhsmc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmh_z_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_y_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmh_y_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_cmh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmh_exploration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmh_y_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nfmc_jax/nfmc_jax/nfomc/nfomc.py\u001b[0m in \u001b[0;36mcmh_exploration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m                                                              \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmh_proposal_cov\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                                                              shape=(self.cmh_z_samples.shape[0],))\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0mcmh_z_proposals\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_scales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0mcmh_z_proposals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmh_z_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jax/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx)\u001b[0m\n\u001b[1;32m   4987\u001b[0m   \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4988\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4989\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4991\u001b[0m \u001b[0;31m# TODO(phawkins): re-enable jit after fixing excessive recompilation for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jax/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx)\u001b[0m\n\u001b[1;32m   4994\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4995\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_static_and_dynamic_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4996\u001b[0;31m   \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index_to_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shared with _scatter_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4997\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jax/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   5086\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_index_to_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5087\u001b[0m   \u001b[0;31m# Remove ellipses and add trailing slice(None)s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5088\u001b[0;31m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_canonicalize_tuple_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5090\u001b[0m   \u001b[0;31m# Check for advanced indexing:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jax/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_canonicalize_tuple_index\u001b[0;34m(arr_ndim, idx)\u001b[0m\n\u001b[1;32m   5361\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen_without_none\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0marr_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5362\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Too many indices for array: {} non-None/Ellipsis indices for dim {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5363\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_without_none\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_ndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5364\u001b[0m   \u001b[0mellipses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5365\u001b[0m   \u001b[0mellipsis_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mellipses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Too many indices for array: 1 non-None/Ellipsis indices for dim 0."
     ]
    }
   ],
   "source": [
    "trace = nfmc_jax.infer_nfomc(log_like_func=log_like_cg, \n",
    "                             log_prior_func=log_flat_prior, \n",
    "                             init_samples=jax_prior_init, \n",
    "                             log_prior_args=log_prior_args, \n",
    "                             inference_mode=inference_mode, \n",
    "                             vmap=vmap, \n",
    "                             parallel_backend=parallel_backend, \n",
    "                             bounds=bounds,\n",
    "                             k_trunc=k_trunc,\n",
    "                             #sinf parameters\n",
    "                             alpha_w=alpha_w,\n",
    "                             NBfirstlayer=NBfirstlayer, \n",
    "                             verbose=verbose,\n",
    "                             interp_nbin=interp_nbin,\n",
    "                             iteration=iteration,\n",
    "                             trainable_qw=trainable_qw,\n",
    "                             sgd_steps=sgd_steps,\n",
    "                             gamma=gamma,\n",
    "                             knots_trainable=knots_trainable,\n",
    "                             optimize_directions=optimize_directions,\n",
    "                             logp_cut=logp_cut,\n",
    "                             random_seed=seed,\n",
    "                             edge_bins=edge_bins,\n",
    "                             Whiten=Whiten,\n",
    "                             #SMC parameters\n",
    "                             nfmc_frac_validate=nfmc_frac_validate,\n",
    "                             min_delta_beta_init=min_delta_beta_init,\n",
    "                             min_delta_beta_final=min_delta_beta_final,\n",
    "                             snf_loss=snf_loss,\n",
    "                             nfmc_snf_only=nfmc_snf_only,\n",
    "                             fixed_beta_schedule=fixed_beta_schedule,\n",
    "                             vanilla_smc=vanilla_smc,\n",
    "                             reg=reg,\n",
    "                             reg1=reg1,\n",
    "                             mh_mode=mh_mode,\n",
    "                             imh_target_acc_rate=imh_target_acc_rate,\n",
    "                             max_imh_steps=max_imh_steps,\n",
    "                             cmh_target_acc_rate=cmh_target_acc_rate,\n",
    "                             max_cmh_steps=max_cmh_steps, \n",
    "                             cmh_p_acc_rate=cmh_p_acc_rate)\n",
    "                             #NERCOME=NERCOME,\n",
    "                             #sinf_resampled=sinf_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c70c9",
   "metadata": {},
   "source": [
    "The output from sampling is stored in the trace object. This is currently a collection of dictionaries containing a lot of output stats and samples for development/debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5104a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dictionary keys.\n",
    "trace.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To inspect the sampling output, we can pull out the q_models.\n",
    "q_models = trace['q_models'][0]\n",
    "q_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were 10 temperature levels. q10_w corresponds to the q fit at beta=1 i.e. the target.\n",
    "q10 = q_models['q10_w']\n",
    "# We can obtain samples from the q model as follows.\n",
    "q_samples, logq = q10.sample(2000, device='cpu')\n",
    "q_samples, logq = q_samples.numpy(), logq.numpy()\n",
    "# And now we can calculate IW of these samples to the target.\n",
    "logp = jax.vmap(lambda x: log_like_cg(x, np.zeros(n), iCov))(q_samples)\n",
    "logw = logp - logq\n",
    "logw -= logsumexp(logw)\n",
    "iw = np.exp(logw)\n",
    "iw /= np.sum(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting up the samples in a corner plot.\n",
    "\n",
    "def plot_triangles(samples, weights, Cov, Ngd=1000, beta_idx=None,\n",
    "                   out_name=None):\n",
    "\n",
    "    names = [\"x%s\"%i for i in range(n)]\n",
    "    labels =  [\"x_%s\"%i for i in range(n)]\n",
    "    \n",
    "    truth = n_mvn.rvs(mean=np.zeros(n), cov=Cov, size=Ngd)\n",
    "    truth_gd = MCSamples(samples=truth,names = names, labels = labels, label='truth')\n",
    "\n",
    "    samples_w = MCSamples(samples=samples, weights=weights,\n",
    "                          names=names, labels=labels, \n",
    "                          label='posterior samples')\n",
    "\n",
    "    # Triangle plot\n",
    "    plt.figure()\n",
    "    g = plots.get_subplot_plotter()\n",
    "    g.triangle_plot([truth_gd, samples_w], filled=True)\n",
    "    plt.show()\n",
    "    if out_name is not None:\n",
    "        g.export(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot the samples without weights.\n",
    "plot_triangles(q_samples, np.ones(len(q_samples)), Cov, \n",
    "               out_name='resampled_sinf_fit_nercome_false_unweighted_qw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the samples with their corresponding IW.\n",
    "plot_triangles(q_samples, iw, Cov, \n",
    "               out_name='resampled_sinf_fit_nercome_false_weighted_qw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cfdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a07120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
