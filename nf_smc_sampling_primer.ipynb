{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a607884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Change to wherever you've copied the repo.\n",
    "sys.path.insert(1, '/home/richard/nfmc_jax/')\n",
    "import nfmc_jax\n",
    "import arviz as az\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import multivariate_normal as n_mvn\n",
    "import chaospy\n",
    "import corner\n",
    "import torch\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import getdist\n",
    "from getdist import plots, MCSamples\n",
    "\n",
    "seed=1234\n",
    "np.random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5958c65",
   "metadata": {},
   "source": [
    "# Sampling a 10d correlated Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93520b",
   "metadata": {},
   "source": [
    "To illustrate the usage of the sampling code, I'll consider a 10-dimensional correlated Gaussian. For now, you'll want to stick with the independent Metropolis-Hastings (MH) update method. Here we proceed as you would with normal SMC, but instead of the normal MH sampling at each temperature level, we draw samples from the fitted q and apply the MH acceptance criterion to these samples (I'll produce some separate notes describing the method in more detail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96040d61",
   "metadata": {},
   "source": [
    "The code requires three key things to get going:\n",
    "\n",
    "1. A set of samples from the prior.\n",
    "2. A function that returns the log-likelihood.\n",
    "3. A function that returns the log-prior.\n",
    "\n",
    "The code uses a Jax backend, so you need to make sure that the log-likelihood and log-prior functions return Jax DeviceArray objects. If you're new to Jax, it's very similar to numpy (basically being numpy + autodiff + some other nice stuff). For a lot of use cases it's as simple as replacing numpy -> jax.numpy. \n",
    "\n",
    "For now, you need to supply the prior samples manually. Given that we're using the Jax backend, it may be possible to interface with numpyro or tensorflow probability models down the line and obtain these automatically from the model object. However, that's currently a future feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be27c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by obtaining some prior samples. I've chosen Unif(-1, 1) in all dimensions.\n",
    "\n",
    "n = 10 # Number of dimensions.\n",
    "n_prior = 200 # Number of prior samples\n",
    "# Note the use of the key here. Jax uses this as opposed to your normal random seed. \n",
    "jax_prior_init = jax.random.uniform(key, shape=(n_prior, n), minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942c5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSD:  True\n",
      "Condition number:  199.08320751596906\n",
      "Op norm:  0.1587924033016887\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is just a function to generate a covariance matrix for the Gaussian as a random sample from a Wishart\n",
    "distribution, whilst ensuring the condition number is not too high.\n",
    "'''\n",
    "def get_icov(n, target=200, iseed=seed, eps=1, scale=50, just_do_it=False):\n",
    "    condition = 0\n",
    "    eigmax = np.inf\n",
    "    this_seed = iseed\n",
    "    if just_do_it:\n",
    "        wish = scipy.stats.wishart(df=n, scale=np.eye(n)*scale, seed=this_seed)\n",
    "        iC = wish.rvs(size=1)\n",
    "        C = np.linalg.inv(iC)\n",
    "        eigs = np.linalg.eigvals(C)\n",
    "        eigmax,eigmin = eigs.max(), eigs.min()\n",
    "        condition = eigmax / eigmin\n",
    "    elif not just_do_it:\n",
    "        while(abs(condition-target)>eps):\n",
    "            wish = scipy.stats.wishart(df=n, scale=np.eye(n)*scale,seed=this_seed)\n",
    "            iC = wish.rvs(size=1)\n",
    "            C = np.linalg.inv(iC)\n",
    "            eigs = np.linalg.eigvals(C)\n",
    "            eigmax,eigmin = eigs.max(),eigs.min()\n",
    "            condition = eigmax / eigmin\n",
    "            this_seed += 1\n",
    "    print(\"PSD: \",np.all(eigs>0))\n",
    "    print(\"Condition number: \", condition)\n",
    "    print(\"Op norm: \", eigmax)\n",
    "    return iC, C\n",
    "\n",
    "iCov, Cov = get_icov(n, eps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc52d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function that returns the log-likelihood. First argument must be the variables you want to sample,\n",
    "followed by any additional arguments.\n",
    "'''\n",
    "def log_like_cg(x, mu_diag=0, icov=iCov):\n",
    "    n=x.shape[-1]\n",
    "    mu = mu_diag * jnp.ones(n)\n",
    "    return - 0.5 * jnp.dot((x - mu), jnp.dot(icov, (x - mu)))\n",
    "'''\n",
    "Define a function that returns the log-prior. As before, the first argument must the the variables you want to \n",
    "sample, followed by any additional arguments.\n",
    "'''\n",
    "def log_flat_prior(x, lower=-1, upper=1):\n",
    "    return jnp.sum(jax.scipy.stats.uniform.logpdf(x, loc=lower, scale=upper-lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101eae22",
   "metadata": {},
   "source": [
    "Below I have defined the key parameters that I think you will need to worry about. If you look at the actual code you'll see a lot more parameters. Some of these are from older versions of the algorithm, and some of these are only relevant for the NFO optimization algorithm. We'll tidy this up at some point, but this should be enough to get you going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6914a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "General parameters\n",
    "------------------\n",
    "In addition to the log-likelihood and prior functions defined above that you'll pass, we have:\n",
    "\n",
    "1. log_like_args: Any additional arguments to be passed to the log-likelihood function as a tuple object.\n",
    "2. log_prior_args: As above but for the prior. Not necessary here, but I've included arguments for illustration.\n",
    "3. inference_mode: Either 'optimization' or 'sampling'. You'll want 'sampling'.\n",
    "4. vmap: If True we evaluate the log-likelihood and prior functions using a vectorizing map (i.e. jax.vmap). \n",
    "   If False we parallelise the computation over available cores. The parallelisation has been tested on NERSC, \n",
    "   but I've pretty much exclusively been using vmap (you may want to parallelise if you have some very expensive \n",
    "   likelihoods).\n",
    "5. parallel_backend: If using parallel evaluations, choose the parallel backend. Can be set to Pool, Client or\n",
    "   MapReduce. Will use multiprocess, dask or sharedmem respectively. If left as None, will use multiprocess.\n",
    "6. bounds: Any hard parameter bounds to pass to SINF/SNF. Passed as array([lower_bounds, upper_bounds]). None\n",
    "   if no hard bounds.\n",
    "7. k_trunc: Clipping parameter for importance weights. IW clipped at <IW>*N^k_trunc. Note, not currently used\n",
    "   with SMC-type sampling.\n",
    "8. random_seed: Random seed for inference.\n",
    "'''\n",
    "log_like_args = ()\n",
    "log_prior_args = ((jnp.array([-1]), jnp.array([1]))) \n",
    "inference_mode = 'sampling'\n",
    "vmap = True\n",
    "parallel_backend = None\n",
    "bounds = np.array([-np.ones(n), np.ones(n)])\n",
    "k_trunc = 0.5\n",
    "random_seed = seed\n",
    "\n",
    "'''\n",
    "SINF/SNF parameters\n",
    "---------------\n",
    "Key SINF/SNF parameters. See GIS.py, SINF.py and optimize.py for code.\n",
    "\n",
    "1. alpha_w: Regularisation parameters for weighted q SINF fits. Passed as a tuple of two values between 0 and 1. \n",
    "   Closer to 1 will give more regularisation. If set to None, SINF uses cross-validation to choose a value.\n",
    "2. NBfirstlayer: Whether to use a Naive Bayes first layer in SINF.\n",
    "3. verbose: Whether you want verbose output from SINF.\n",
    "4. interp_nbin: Number of spline knots for rational quadratic splines in SINF. \n",
    "5. trainable_qw: Whether to run SNF after SINF. This was relevant when SINF and SNF were contained in the same\n",
    "   method. There are separate methods now, and I'm using this so I can more easily change things. You should\n",
    "   probably leave this as False for now. It won't break anything if you set it as True. Just means you'll run \n",
    "   SNF one more time than you think.\n",
    "6. sgd_steps: Number of stochastic gradient descent steps to take for SNF training.\n",
    "7. gamma: Parameter in loss function 1 (log E(q^(-gamma)*(p - Zq)^2)). \n",
    "8. knots_trainable: Only relevant if trainable_qw is True, in which case this overwrites interp_nbin.\n",
    "9. optimize_directions: Whether we optimize directions in SNF.\n",
    "10. logp_cut: logp threshold to be applied during SNF training. Currently I don't impose a cut.\n",
    "11. edge_bins: Number of spline knots at the boundary.\n",
    "12. Whiten: Whether to include a whitening layer for SINF.\n",
    "13. iteration: Maximum number of SINF layers.\n",
    "'''\n",
    "#sinf parameters\n",
    "alpha_w = (0.9, 0.9)\n",
    "NBfirstlayer = True\n",
    "verbose = False\n",
    "interp_nbin = 5\n",
    "iteration = 5\n",
    "trainable_qw = False # Just leave this for now.\n",
    "sgd_steps = 10\n",
    "gamma = 0\n",
    "knots_trainable = 5\n",
    "optimize_directions = False\n",
    "logp_cut = None\n",
    "edge_bins = 0\n",
    "Whiten = True\n",
    "iteration = 5\n",
    "\n",
    "'''\n",
    "SMC parameters\n",
    "--------------\n",
    "Parameters used for SMC and MH exploration strategies.\n",
    "\n",
    "1. nfmc_frac_validate: Fraction of samples used in validation for SINF/SNF.\n",
    "2. min_delta_beta_init: Minimum relative change in beta at start.\n",
    "3. min_delta_beta_final: Minimum relative change in beta at end. delta_beta decays to this between start and end.\n",
    "4. snf_loss: Specify the SNF loss function. (0) E(q^(-gamma)*(p - Zq)^2), (1) log E(q^(-gamma)*(p - Zq)^2),\n",
    "   (2) sum((logp-logq-logZ)^2), (3) -E(logq), (4) log(E((q/q_uw)(p/q - Z)^2)).\n",
    "5. nfmc_snf_only: Whether to only use trainable SNF updates to q after the first q fit.\n",
    "6. fixed_beta_schedule: Can specify a fixed beta schedule as a list of beta values.\n",
    "7. vanilla_smc: Whether to use the SMC+MH exploration strategy. This is currently the focus for sampling.\n",
    "   Old option was to intialise using NFO optimization.\n",
    "8. reg: SNF regularisation parameter.\n",
    "9. reg1: SNF regularisation parameter.\n",
    "10. mh_mode: Specify 'IMH' for independent MH exploration, 'CMH' for latent space MH, 'both' for using both.\n",
    "    CMH is still being tested so stick with 'IMH' for now.\n",
    "11. imh_target_acc_rate: Target acceptance fraction for original samples in IMH.\n",
    "12. max_imh_steps: Maximum number of IMH iterations at a given temperature level.\n",
    "13. cmh_target_acc_rate: Target acceptance rate for CMH exploration.\n",
    "14. max_cmh_steps: Maximum number of steps in a CMH chain.\n",
    "15. cmh_p_acc_rate: Parameter used for tuning number of CMH steps. Between 0 and 1. Closer to 1 results in more\n",
    "    steps.\n",
    "'''\n",
    "nfmc_frac_validate = 0.1\n",
    "min_delta_beta_init = 0.01\n",
    "min_delta_beta_final = 0.01\n",
    "snf_loss = 2\n",
    "nfmc_snf_only = False\n",
    "fixed_beta_schedule = None\n",
    "vanilla_smc = True\n",
    "reg = 0.9\n",
    "reg1 = 0.9\n",
    "mh_mode = 'IMH'\n",
    "imh_target_acc_rate = 0.5\n",
    "max_imh_steps = 25\n",
    "cmh_target_acc_rate = 0.234\n",
    "max_cmh_steps = 25\n",
    "cmh_p_acc_rate = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ddc9d7",
   "metadata": {},
   "source": [
    "Now we can run our 10d Gaussian example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a84d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference mode is sampling. Maximum beta is set to 1.\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.0037841796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/nfmc_jax/nfmc_jax/nfomc/nfomc.py:383: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448222085/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  weight_train=torch.from_numpy(np.asarray(self.train_weights[fit_idx, ...])),\n",
      "/home/richard/nfmc_jax/nfmc_jax/sinf/SINF.py:292: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448222085/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2500.)\n",
      "  D, E = torch.symeig(covariance, eigenvectors=True)\n",
      "/home/richard/nfmc_jax/nfmc_jax/sinf/SINF.py:495: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448222085/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:1940.)\n",
      "  Q, R = torch.qr(ATi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for trainable qw = 0.08674800399999949\n",
      "Acceptance rate at IMH step 0: 0.46\n",
      "Acceptance rate of original samples: 0.46\n",
      "Time taken for trainable qw = 0.06814941200000035\n",
      "Acceptance rate at IMH step 1: 0.465\n",
      "Acceptance rate of original samples: 0.66\n",
      "Time taken for trainable qw = 0.07067391400000034\n",
      "Current qw ESS to target: 79\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.010485343635082245\n",
      "Time taken for trainable qw = 0.1522362639999999\n",
      "Acceptance rate at IMH step 0: 0.34\n",
      "Acceptance rate of original samples: 0.34\n",
      "Time taken for trainable qw = 0.15773594800000001\n",
      "Acceptance rate at IMH step 1: 0.425\n",
      "Acceptance rate of original samples: 0.63\n",
      "Time taken for trainable qw = 0.1787534070000003\n",
      "Current qw ESS to target: 38\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.023114098778023617\n",
      "Time taken for trainable qw = 0.15627911999999977\n",
      "Acceptance rate at IMH step 0: 0.315\n",
      "Acceptance rate of original samples: 0.315\n",
      "Time taken for trainable qw = 0.1613739590000005\n",
      "Acceptance rate at IMH step 1: 0.445\n",
      "Acceptance rate of original samples: 0.625\n",
      "Time taken for trainable qw = 0.14804443399999911\n",
      "Current qw ESS to target: 42\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.0429022633166225\n",
      "Time taken for trainable qw = 0.15561209899999895\n",
      "Acceptance rate at IMH step 0: 0.34\n",
      "Acceptance rate of original samples: 0.34\n",
      "Time taken for trainable qw = 0.16583451400000015\n",
      "Acceptance rate at IMH step 1: 0.38\n",
      "Acceptance rate of original samples: 0.565\n",
      "Time taken for trainable qw = 0.15605459800000077\n",
      "Current qw ESS to target: 55\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.06942055540303058\n",
      "Time taken for trainable qw = 0.15304502999999947\n",
      "Acceptance rate at IMH step 0: 0.355\n",
      "Acceptance rate of original samples: 0.355\n",
      "Time taken for trainable qw = 0.14732175299999994\n",
      "Acceptance rate at IMH step 1: 0.465\n",
      "Acceptance rate of original samples: 0.635\n",
      "Time taken for trainable qw = 0.16669833799999978\n",
      "Current qw ESS to target: 28\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.10995518241361149\n",
      "Time taken for trainable qw = 0.15158015299999938\n",
      "Acceptance rate at IMH step 0: 0.305\n",
      "Acceptance rate of original samples: 0.305\n",
      "Time taken for trainable qw = 0.15843747299999933\n",
      "Acceptance rate at IMH step 1: 0.485\n",
      "Acceptance rate of original samples: 0.61\n",
      "Time taken for trainable qw = 0.16452061600000079\n",
      "Current qw ESS to target: 59\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.19762815979188636\n",
      "Time taken for trainable qw = 0.15331380399999972\n",
      "Acceptance rate at IMH step 0: 0.315\n",
      "Acceptance rate of original samples: 0.315\n",
      "Time taken for trainable qw = 0.15753749699999986\n",
      "Acceptance rate at IMH step 1: 0.51\n",
      "Acceptance rate of original samples: 0.655\n",
      "Time taken for trainable qw = 0.1514418519999996\n",
      "Current qw ESS to target: 59\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.3032358848040805\n",
      "Time taken for trainable qw = 0.16565428600000054\n",
      "Acceptance rate at IMH step 0: 0.36\n",
      "Acceptance rate of original samples: 0.36\n",
      "Time taken for trainable qw = 0.1793640320000005\n",
      "Acceptance rate at IMH step 1: 0.565\n",
      "Acceptance rate of original samples: 0.705\n",
      "Time taken for trainable qw = 0.14914202900000006\n",
      "Current qw ESS to target: 50\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.4772204864599121\n",
      "Time taken for trainable qw = 0.14516893400000086\n",
      "Acceptance rate at IMH step 0: 0.32\n",
      "Acceptance rate of original samples: 0.32\n",
      "Time taken for trainable qw = 0.1639870239999972\n",
      "Acceptance rate at IMH step 1: 0.54\n",
      "Acceptance rate of original samples: 0.685\n",
      "Time taken for trainable qw = 0.17737076800000295\n",
      "Current qw ESS to target: 83\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 0.7211031429253169\n",
      "Time taken for trainable qw = 0.19536980599999865\n",
      "Acceptance rate at IMH step 0: 0.365\n",
      "Acceptance rate of original samples: 0.365\n",
      "Time taken for trainable qw = 0.1866658369999996\n",
      "Acceptance rate at IMH step 1: 0.605\n",
      "Acceptance rate of original samples: 0.75\n",
      "Time taken for trainable qw = 0.14500334999999964\n",
      "Current qw ESS to target: 98\n",
      "min_delta_beta = 0.01\n",
      "ESS for samples from most recent qw = 100\n",
      "Updated beta = 1\n",
      "Time taken for trainable qw = 0.15785935400000284\n",
      "Acceptance rate at IMH step 0: 0.34\n",
      "Acceptance rate of original samples: 0.34\n",
      "Time taken for trainable qw = 0.16275472799999946\n",
      "Acceptance rate at IMH step 1: 0.73\n",
      "Acceptance rate of original samples: 0.83\n",
      "Time taken for trainable qw = 0.1562888419999986\n",
      "Current qw ESS to target: 131\n",
      "Time for vanilla SMC = 17.069664387\n"
     ]
    }
   ],
   "source": [
    "trace = nfmc_jax.infer_nfomc(log_like_func=log_like_cg, \n",
    "                             log_prior_func=log_flat_prior, \n",
    "                             init_samples=jax_prior_init, \n",
    "                             log_prior_args=log_prior_args, \n",
    "                             inference_mode=inference_mode, \n",
    "                             vmap=vmap, \n",
    "                             parallel_backend=parallel_backend, \n",
    "                             bounds=bounds,\n",
    "                             k_trunc=k_trunc,\n",
    "                             #sinf parameters\n",
    "                             alpha_w=alpha_w,\n",
    "                             NBfirstlayer=NBfirstlayer, \n",
    "                             verbose=verbose,\n",
    "                             interp_nbin=interp_nbin,\n",
    "                             iteration=iteration,\n",
    "                             trainable_qw=trainable_qw,\n",
    "                             sgd_steps=sgd_steps,\n",
    "                             gamma=gamma,\n",
    "                             knots_trainable=knots_trainable,\n",
    "                             optimize_directions=optimize_directions,\n",
    "                             logp_cut=logp_cut,\n",
    "                             random_seed=seed,\n",
    "                             edge_bins=edge_bins,\n",
    "                             Whiten=Whiten,\n",
    "                             #SMC parameters\n",
    "                             nfmc_frac_validate=nfmc_frac_validate,\n",
    "                             min_delta_beta_init=min_delta_beta_init,\n",
    "                             min_delta_beta_final=min_delta_beta_final,\n",
    "                             snf_loss=snf_loss,\n",
    "                             nfmc_snf_only=nfmc_snf_only,\n",
    "                             fixed_beta_schedule=fixed_beta_schedule,\n",
    "                             vanilla_smc=vanilla_smc,\n",
    "                             reg=reg,\n",
    "                             reg1=reg1,\n",
    "                             mh_mode=mh_mode,\n",
    "                             imh_target_acc_rate=imh_target_acc_rate,\n",
    "                             max_imh_steps=max_imh_steps,\n",
    "                             cmh_target_acc_rate=cmh_target_acc_rate,\n",
    "                             max_cmh_steps=max_cmh_steps, \n",
    "                             cmh_p_acc_rate=cmh_p_acc_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cc40c",
   "metadata": {},
   "source": [
    "The output from sampling is stored in the trace object. This is currently a collection of dictionaries containing a lot of output stats and samples for development/debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512aed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['log_evidence', 'q_samples', 'importance_weights', 'logp', 'logq', 'train_logp', 'train_logq', 'logZ', 'q_models', 'q_ess', 'train_ess', 'total_ess', 'N', 'min_var_bws', 'min_pq_bws', '_t_sampling'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dictionary keys.\n",
    "trace.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e3d272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['q1_w', 'q2_w', 'q3_w', 'q4_w', 'q5_w', 'q6_w', 'q7_w', 'q8_w', 'q9_w', 'q10_w'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To inspect the sampling output, we can pull out the q_models.\n",
    "q_models = trace['q_models'][0]\n",
    "q_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1151411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were 10 temperature levels. q10_w corresponds to the q fit at beta=1 i.e. the target.\n",
    "q10 = q_models['q10_w']\n",
    "# We can obtain samples from the q model as follows.\n",
    "q_samples, logq = q10.sample(2000, device='cpu')\n",
    "q_samples, logq = q_samples.numpy(), logq.numpy()\n",
    "# And now we can calculate IW of these samples to the target.\n",
    "logp = jax.vmap(lambda x: log_like_cg(x, np.zeros(n), iCov))(q_samples)\n",
    "logw = logp - logq\n",
    "logw -= logsumexp(logw)\n",
    "iw = np.exp(logw)\n",
    "iw /= np.sum(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cfe2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting up the samples in a corner plot.\n",
    "\n",
    "def plot_triangles(samples, weights, Cov, Ngd=1000, beta_idx=None,\n",
    "                   out_name=None):\n",
    "\n",
    "    names = [\"x%s\"%i for i in range(n)]\n",
    "    labels =  [\"x_%s\"%i for i in range(n)]\n",
    "    \n",
    "    truth = n_mvn.rvs(mean=np.zeros(n), cov=Cov, size=Ngd)\n",
    "    truth_gd = MCSamples(samples=truth,names = names, labels = labels, label='truth')\n",
    "\n",
    "    samples_w = MCSamples(samples=samples, weights=weights,\n",
    "                          names=names, labels=labels, \n",
    "                          label='posterior samples')\n",
    "\n",
    "    # Triangle plot\n",
    "    plt.figure()\n",
    "    g = plots.get_subplot_plotter()\n",
    "    g.triangle_plot([truth_gd, samples_w], filled=True)\n",
    "    plt.show()\n",
    "    if out_name is not None:\n",
    "        g.export(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c5b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed no burn in\n",
      "Removed no burn in\n"
     ]
    }
   ],
   "source": [
    "# First plot the samples without weights.\n",
    "plot_triangles(q_samples, np.ones(len(q_samples)), Cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the samples with their corresponding IW.\n",
    "plot_triangles(q_samples, iw, Cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d28a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0722f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
